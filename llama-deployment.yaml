apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama3-inference
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama3
  template:
    metadata:
      labels:
        app: llama3
    spec:
      # 1. The VIP Pass (Toleration)
      # This allows the pod to land on the "tainted" GPU node
      tolerations:
      - key: "accelerator"
        operator: "Equal"
        value: "gpu"
        effect: "NoSchedule"

      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        
        # 2. The Trigger (Resource Request)
        # K8s sees this, realizes it has 0 GPUs, and wakes up Karpenter
        resources:
          limits:
            nvidia.com/gpu: 1

        # 3. The Token (From Secret)
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-token
              key: token
        
        # 4. The Command (Same optimization flags as Project 1)
        command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
        args: 
          - "--model"
          - "meta-llama/Meta-Llama-3.1-8B-Instruct"
          - "--max-model-len"
          - "8192"
          - "--gpu-memory-utilization"
          - "0.95"
          - "--port"
          - "8000"